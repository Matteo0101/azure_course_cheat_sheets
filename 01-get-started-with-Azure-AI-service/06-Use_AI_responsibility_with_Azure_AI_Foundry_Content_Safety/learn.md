# Azure AI Foundry Content Safety Guide

## ğŸ” Problem
You need to detect and moderate harmful, offensive, or inappropriate contentâ€”whether user-generated or AI-generatedâ€”across various formats (text, image, multimodal) and languages, ensuring compliance with regulations and protecting user safety and brand integrity.

## âœ… Solution with Azure
Use **Azure AI Foundry Content Safety**, an AI-powered content moderation service that analyzes text, images, and multimodal inputs to detect content that falls into four core categories:
* **Violence**
* **Hate speech**
* **Sexual content**
* **Self-harm**

It replaces the deprecated Azure Content Moderator and provides more advanced, multilingual and multimodal content safety tools.

## ğŸ§© Required Components
* **Azure AI Foundry Portal** (access via Azure portal)
* **Content Safety Studio** or **Azure AI Content Studio**
* **Moderation APIs** for:
   * Text content
   * Image content
   * Multimodal content (OCR + analysis)
* **Custom Categories** (optional)
* **Prompt Shields API** for LLM prompt protection
* **Protected Material Detection**
* **Groundedness Detection API**
* **Safety System Messages**

## ğŸ—ï¸ Architecture / Development

### ğŸ§  Text Moderation
* Input analyzed by NLP across four categories
* Severity levels (0â€“6) returned per category
* Use **blocklists** for specific terms
* API returns structured moderation data

### ğŸ–¼ï¸ Image Moderation
* Uses **Florence foundation model**
* Returns severity: *safe*, *low*, *high*
* Developer sets threshold: *low*, *medium*, *high*
* Evaluated per category

### ğŸ§¾ Multimodal Moderation
* Uses OCR to extract and analyze text within images
* Same four categories evaluated

### ğŸ›¡ï¸ Prompt Shields
* Blocks prompt-based jailbreaks on LLMs
* Applies to user input and embedded document content

### ğŸ§¾ Protected Material Detection
* Flags copyright violations in AI-generated content

### ğŸ“‰ Groundedness Detection
* Compares LLM responses to source data
* Optionally returns **reasoning** for ungrounded flags

### ğŸ›ï¸ Custom Categories
* Define categories using positive/negative examples
* Train a model for customized moderation

## ğŸ“Œ Best Practices / Considerations
* **Test on real data** before deploying
* Continuously **monitor accuracy** post-deployment
* Use human moderators for edge cases and appeals
* Communicate clearly to users why content is flagged
* Understand AI limits: possibility of **false positives/negatives**
* Evaluate using:
   * âœ… True Positives
   * âŒ False Positives
   * âœ… True Negatives
   * âŒ False Negatives

## â“ Simulated Exam Questions
1. ğŸ§  *How does Azure AI Foundry Content Safety determine if text should be blocked or approved?* â¤ By assigning severity levels (0â€“6) across four categories: violence, hate, sexual content, and self-harm.

2. ğŸ§  *What model powers image analysis in Foundry Content Safety?* â¤ Florence foundation model.

3. ğŸ§  *How does the system evaluate multimodal content?* â¤ It uses OCR to extract text from images, then analyzes both the image and text across the four categories.

4. ğŸ§  *Which service should you use to prevent prompt-based jailbreaks in LLM inputs?* â¤ Prompt Shields.

5. ğŸ§  *What should you do before deploying Foundry Content Safety in a live environment?* â¤ Test on real data and plan continuous monitoring.

6. ğŸ§  *Which functionality detects grounded vs. ungrounded responses from LLMs?* â¤ Groundedness Detection.

7. ğŸ§  *Can you define custom moderation rules? How?* â¤ Yes, using the **Custom Categories** feature by training with example content.

8. ğŸ§  *What are the severity levels used for image moderation?* â¤ Safe, Low, High â€” combined with threshold settings (Low, Medium, High) to determine action.